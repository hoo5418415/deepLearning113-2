# 多臂強盜 (Multi-Armed Bandit) 算法詳解

本報告介紹四種常見的多臂強盜算法，分別為 **Epsilon-Greedy**、**UCB (Upper Confidence Bound)**、**Softmax** 與 **Thompson Sampling**。本文內容包含：

- 每個算法對應的 ChatGPT Prompt（用以解釋算法邏輯）
- 使用 LaTeX 格式排版的算法公式
- 詳細的結果分析，包括從「空間」與「時間」兩個層面的探討
- 各算法的優勢與限制，以及在不同情境下的表現比較

---

## 1. Epsilon-Greedy 算法

### ChatGPT Prompt

請詳細解釋 Epsilon-Greedy 算法中如何利用參數 $\epsilon$ 來在探索（隨機選擇動作）與利用（選擇當前最佳動作）之間取得平衡，並以簡單範例說明其動作更新過程與累積獎勵的變化。

### 算法公式

$$
a_t = 
\begin{cases}
\text{隨機選擇一個動作}, & \text{if } rand() < \epsilon, \\
\arg\max_{a} Q_t(a), & \text{otherwise}
\end{cases}
$$

$$
Q_{t+1}(a) = Q_t(a) + \alpha \left( r_t - Q_t(a) \right)
$$

**說明：**  
- $Q_t(a)$：在時間 $t$ 時對動作 $a$ 的估計值  
- $r_t$：該次選擇獲得的獎勵  
- $\alpha$：學習率（或根據樣本平均設定為 $\alpha = \frac{1}{N_t(a)}$）

### 結果分析

- **時間分析：**  
  初期由於各臂估計值不穩定，因此較多進行隨機探索；隨著時間增加，更新公式使估計值逐漸收斂，最終主要依據較高的 $Q_t(a)$ 進行利用，累積獎勵呈現上升趨勢。

- **空間分析：**  
  每個臂根據其獲取的獎勵數據得到不同估計值，隨著選擇次數增加，估計值更接近真實獎勵，從而最佳臂最終獲得較多選擇，而探索階段亦能覆蓋整個行動空間。

- **優勢與限制：**  
  - **優勢：** 結構簡單、易於實現；能在探索與利用之間取得基本平衡。  
  - **限制：** 固定的 $\epsilon$ 參數可能導致過多或不足的探索，缺乏自適應性（尤其在環境變化時）。

---

## 2. UCB (Upper Confidence Bound) 算法

### ChatGPT Prompt

請解釋 UCB 算法如何透過置信上界（Upper Confidence Bound）來平衡探索與利用，並詳細說明公式中的  
$$
c \sqrt{\frac{\ln t}{N_t(a)}}
$$  
部分如何為未充分探索的臂提供額外探索獎勵，進而改善累積獎勵表現。

### 算法公式

$$
a_t = \arg\max_{a} \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]
$$

**說明：**  
- $Q_t(a)$：臂 $a$ 在時間 $t$ 的平均獎勵估計  
- $N_t(a)$：臂 $a$ 被選擇的次數  
- $c$：調控探索程度的參數

### 結果分析

- **時間分析：**  
  初期每個臂皆被拉取一次獲取初步估計；隨後未充分探索的臂因 $\sqrt{\frac{\ln t}{N_t(a)}}$ 項得到較高置信上界，促使持續探索。隨著累計次數增加，較好臂的 $Q_t(a)$ 顯現優勢，最終最佳臂被穩定選擇。

- **空間分析：**  
  置信上界項確保初期評估值較低的臂（因被選次數少）也能獲得探索機會，從而在整個動作空間中達到均衡探索，避免部分臂被忽略。

- **優勢與限制：**  
  - **優勢：** 理論上提供對數尺度的累積懊悔界限；能動態平衡探索與利用。  
  - **限制：** 參數 $c$ 的選擇對結果有明顯影響；初期數據不足時較敏感，在非平穩環境中可能需特殊調整。

---

## 3. Softmax 算法

### ChatGPT Prompt

請詳細說明 Softmax 算法如何根據動作的估計值分配概率，利用概率分布進行動作選擇；並討論溫度參數 $\tau$ 如何影響探索與利用的平衡，請以不同 $\tau$ 值下的實例說明累積獎勵變化趨勢。

### 算法公式

$$
P(a) = \frac{e^{Q_t(a)/\tau}}{\sum_{b} e^{Q_t(b)/\tau}}
$$

**說明：**  
- $Q_t(a)$：動作 $a$ 的估計獎勵  
- $\tau$：溫度參數，決定動作選擇的隨機性。高 $\tau$ 使選擇概率分布更平滑，促進探索；低 $\tau$ 則使決策更偏向利用最佳臂。

### 結果分析

- **時間分析：**  
  初期採用較高的 $\tau$ 值能使各臂均被探索；隨著時間進展，若設定較低 $\tau$，則演算法更依賴當前估計值，進而迅速聚焦最佳臂並提升累積獎勵。

- **空間分析：**  
  溫度參數使得各臂根據相對估計值獲得不同的選擇機率，即使表現較低的臂也有一定被選機會，保證全域動作空間均衡探索。

- **優勢與限制：**  
  - **優勢：** 利用平滑概率分布進行行動選擇，靈活調整探索與利用比例。  
  - **限制：** 合適的 $\tau$ 參數需根據具體問題調整，不當設定可能導致過度探索或過早利用。

---

## 4. Thompson Sampling 算法

### ChatGPT Prompt

請解釋 Thompson Sampling 算法如何利用貝葉斯後驗分佈進行動作選擇，並闡述在 Bernoulli 問題中，如何根據  
$$
\theta_a \sim \mathrm{Beta}(S_a+1, F_a+1)
$$  
進行決策，從而在不確定性控制與累積獎勵方面展現優勢。

### 算法公式

$$
a_t = \arg\max_{a} \ \theta_a \quad \text{where} \quad \theta_a \sim \mathrm{Beta}(S_a+1, F_a+1)
$$

**說明：**  
- $S_a$：臂 $a$ 的成功次數  
- $F_a$：臂 $a$ 的失敗次數  
- $\theta_a$：從 Beta 後驗分佈中抽樣得到的參數，反映該臂的真實獎勵潛力

### 結果分析

- **時間分析：**  
  初期由於成功與失敗次數較少，各臂後驗分佈較為分散，導致均衡探索；隨著數據累積，後驗分佈逐漸收斂，使得最佳臂因分佈偏向較高獎勵而頻繁被選中，累積獎勵逐步上升。

- **空間分析：**  
  每個臂維護獨立的 Beta 後驗分佈，根據各自歷史數據動態調整不確定性，確保多臂中各臂均獲得足夠探索機會，避免早期排除潛在優勢臂。

- **優勢與限制：**  
  - **優勢：** 透過貝葉斯方法自適應平衡探索與利用，特別適用於數據稀疏或高度不確定性環境。  
  - **限制：** 模型構建及先驗分佈設計較為複雜；對於非 Bernoulli 問題可能需開發額外後驗更新機制。

---

## 綜合比較與討論

### 時間層面

- **探索階段：**  
  所有算法在初期均展現強烈探索能力；隨著時間增加，資料累積使得：
  - **Epsilon-Greedy** 與 **Softmax** 可能仍保留部分隨機性，
  - **UCB** 與 **Thompson Sampling** 則更快聚焦最佳臂並穩定累積高獎勵。

### 空間層面

- **動作空間探索：**  
  - **Epsilon-Greedy**：固定機率的隨機探索，可能未能充分評估所有臂。  
  - **UCB**：透過置信上界，較少被選臂得到額外補償。  
  - **Softmax**：利用平滑概率分布保證每個臂均有被選機會。  
  - **Thompson Sampling**：依據後驗分佈動態調整，能靈活捕捉各臂表現差異。

### 優勢與限制總結

- **Epsilon-Greedy:**  
  - **優勢：** 簡單易實現。  
  - **限制：** 固定 $\epsilon$ 缺乏自適應性，可能導致不必要的隨機行為。

- **UCB:**  
  - **優勢：** 理論上給出較好的累積懊悔上界，能動態平衡探索與利用。  
  - **限制：** 對參數 $c$ 敏感；初期數據不足時效果可能受影響。

- **Softmax:**  
  - **優勢：** 透過概率分布柔性分配選擇機會，有效避免單一臂過度利用。  
  - **限制：** 溫度參數 $\tau$ 必須謹慎調整，不當設定可能影響收斂速度。

- **Thompson Sampling:**  
  - **優勢：** 基於貝葉斯更新，自適應調整探索與利用，尤其在數據稀疏環境中表現突出。  
  - **限制：** 模型及先驗設計較複雜；對非 Bernoulli 問題可能需要額外修正。

---

### 總結
本文詳細說明了四種多臂強盜算法的核心邏輯與數學基礎，並通過 LaTeX 排版的公式、清晰的 ChatGPT Prompt 以及從「空間」與「時間」兩個層面的詳細結果分析
探討了各算法在探索與利用上的平衡機制及其優缺點。該報告可作為進一步研究或實際應用中的參考資料。